use std::collections::HashMap;
use std::fmt::{Display, Formatter};
use std::ops::Rem;
use std::sync::Arc;
use std::time::Instant;

use log::{debug, info, warn};
use rand::prelude::SliceRandom;
use rand::RngCore;
use serde::{Deserialize, Serialize};

use crate::algorithms::{Algorithm, ExportHistory, NSGA2, StoppingConditionType};
use crate::core::{DataValue, Individual, Individuals, OError, Population, Problem};
use crate::core::utils::get_rng;
use crate::operators::{
    Crossover, Mutation, ParetoConstrainedDominance, PolynomialMutation, PolynomialMutationArgs,
    Selector, SimulatedBinaryCrossover, SimulatedBinaryCrossoverArgs, TournamentSelector,
};
use crate::utils::{
    argmin, argmin_by, DasDarren1998, fast_non_dominated_sort, index_of,
    LinearSolverTolerance, NumberOfPartitions, perpendicular_distance, solve_linear_system, vector_max,
    vector_min,
};

/// The data key where the normalised objectives are stored for each [`Individual`].
const NORMALISED_OBJECTIVE_KEY: &str = "normalised_objectives";

/// The data key where the perpendicular distance to a reference point is stored for each [`Individual`].
const MIN_DISTANCE: &str = "distance";

/// The data key where the reference point with [`MIN_DISTANCE`] is stored for each [`Individual`].
const REF_POINT: &str = "reference_point";

/// The data key where the reference point index for [`REF_POINT`] is stored.
const REF_POINT_INDEX: &str = "reference_point_index";

/// The type for the number of individuals in the population.
#[derive(Serialize, Deserialize, Clone, Debug)]
pub enum Nsga3NumberOfIndividuals {
    /// The number of individuals are set equal to the number of reference points.
    EqualToReferencePointCount,
    /// Set a custom number of individuals. This must be larger than the number of reference points
    /// generated by setting the [`NSGA3Arg::number_of_partitions`].
    Custom(usize),
}

/// Input arguments for the NSGA3 algorithm.
#[derive(Serialize, Deserialize, Clone, Debug)]
pub struct NSGA3Arg {
    /// The number of individuals in the population.
    pub number_of_individuals: Nsga3NumberOfIndividuals,
    /// The number of partitions to use to calculate the reference points or weight.
    pub number_of_partitions: NumberOfPartitions,
    /// The options of the Simulated Binary Crossover (SBX) operator. This operator is used to
    /// generate new children by recombining the variables of parent solutions. This defaults to
    /// [`SimulatedBinaryCrossoverArgs::default()`].
    /// NOTE: it is advisable to use a large `distribution_index` to prevent the problem explained in
    /// Section IIa point #3 in the paper. With many objectives, "two distant parent solutions are
    /// likely to produce offspring solutions that are also distant from parents", which should be
    /// prevented.
    pub crossover_operator_options: Option<SimulatedBinaryCrossoverArgs>,
    /// The options to Polynomial Mutation (PM) operator used to mutate the variables of an
    /// individual. This defaults to [`PolynomialMutationArgs::default()`],
    /// with a distribution index or index parameter of `20` and variable probability equal `1`
    /// divided by the number of real variables in the problem (i.e., each variable will have the
    /// same probability of being mutated).
    pub mutation_operator_options: Option<PolynomialMutationArgs>,
    /// The condition to use when to terminate the algorithm.
    pub stopping_condition: StoppingConditionType,
    /// Whether the objective and constraint evaluation in [`Problem::evaluator`] should run
    /// using threads. If the evaluation function takes a long time to run and return the updated
    /// values, it is advisable to set this to `true`. This defaults to `true`.
    pub parallel: Option<bool>,
    /// The options to configure the individual's history export. When provided, the algorithm will
    /// save objectives, constraints and solutions to a file each time the generation increases by
    /// a given step. This is useful to track convergence and inspect an algorithm evolution.
    pub export_history: Option<ExportHistory>,
    /// The seed used in the random number generator (RNG). You can specify a seed in case you want
    /// to try to reproduce results. NSGA2 is a stochastic algorithm that relies on a RNG at
    /// different steps (when population is initially generated, during selection, crossover and
    /// mutation) and, as such, may lead to slightly different solutions. The seed is randomly
    /// picked if this is `None`.
    pub seed: Option<u64>,
}

/// The Non-dominated Sorting Genetic Algorithm (NSGA3).
///
/// Implemented based on:
/// > K. Deb and H. Jain, "An Evolutionary Many-Objective Optimization Algorithm Using
/// Reference-Point-Based Non-dominated Sorting Approach, Part I: Solving Problems With Box
/// Constraints," in IEEE Transactions on Evolutionary Computation, vol. 18, no. 4, pp. 577-601,
/// Aug. 2014, doi: 10.1109/TEVC.2013.2281535
///
/// See: <https://10.1109/TEVC.2013.2281535>.
pub struct NSGA3 {
    /// The number of individuals to use in the population.
    number_of_individuals: usize,
    /// The vector of reference points
    reference_points: Vec<Vec<f64>>,
    /// The ideal point coordinates when the algorithm starts up to the current evolution
    ideal_point: Vec<f64>,
    /// The population with the solutions.
    population: Population,
    /// The problem being solved.
    problem: Arc<Problem>,
    /// The operator to use to select the individuals for reproduction. This is a binary tournament
    /// selector ([`TournamentSelector`]) with the [`ParetoConstrainedDominance`] comparison operator.
    selector_operator: TournamentSelector<ParetoConstrainedDominance>,
    /// The SBX operator to use to generate a new children by recombining the variables of parent
    /// solutions.
    crossover_operator: SimulatedBinaryCrossover,
    /// The PM operator to use to mutate the variables of an individual.
    mutation_operator: PolynomialMutation,
    /// The evolution step.
    generation: usize,
    /// The stopping condition.
    stopping_condition: StoppingConditionType,
    /// The time when the algorithm started.
    start_time: Instant,
    /// The configuration struct to export the algorithm history.
    export_history: Option<ExportHistory>,
    /// Whether the evaluation should run using threads
    parallel: bool,
    /// The seed to use.
    rng: Box<dyn RngCore>,
    /// The algorithm options
    args: NSGA3Arg,
}

impl Display for NSGA3 {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        f.write_str(self.name().as_str())
    }
}

impl NSGA3 {
    /// Initialise the NSGA3 algorithm.
    ///
    /// # Arguments
    ///
    /// * `problem`: The problem being solved.
    /// * `args`: The [`NSGA3Arg`] arguments to customise the algorithm behaviour.
    ///
    /// returns: `NSGA3`.
    pub fn new(problem: Problem, options: NSGA3Arg) -> Result<Self, OError> {
        let name = "NSGA3".to_string();
        let nsga3_args = options.clone();

        let das_darren = DasDarren1998::new(
            problem.number_of_objectives(),
            &options.number_of_partitions,
        )?;
        let reference_points = das_darren.get_weights();
        info!(
            "Created {} reference directions",
            das_darren.number_of_points()
        );

        // create the population
        let mut number_of_individuals = match options.number_of_individuals {
            Nsga3NumberOfIndividuals::EqualToReferencePointCount => reference_points.len(),
            Nsga3NumberOfIndividuals::Custom(count) => {
                if count < 3 {
                    return Err(OError::AlgorithmInit(
                        name,
                        "The population size must have at least 3 individuals".to_string(),
                    ));
                }
                // add tolerance (for example Deb et al. sometimes uses pop + 1 = ref_point)
                if count - 1 > das_darren.number_of_points() as usize {
                    return Err(OError::AlgorithmInit(
                        name,
                        format!(
                            concat!(
                            "The number of individuals ({}) must be larger than the number of ",
                            "reference points ({}) to prevent unexpected behaviours. It is always ",
                            "advised to associate at least one individual to a reference point"),
                            count,
                            das_darren.number_of_points()
                        ),
                    ));
                }
                count
            }
        };
        debug!("Population size set to {}", number_of_individuals);

        // force the population size as multiple of 2 so that the new number of generated offsprings
        // matches `number_of_individuals`
        if number_of_individuals.rem(2) != 0 {
            number_of_individuals -= 1;
            warn!(
                "The population size was reduced to {} so that it is a multiple of 2",
                number_of_individuals
            );
        }

        let problem = Arc::new(problem);
        let population = Population::init(problem.clone(), number_of_individuals);
        info!("Created initial random population");

        let selector_operator = TournamentSelector::<ParetoConstrainedDominance>::new(2);
        let mutation_options = match options.mutation_operator_options {
            Some(o) => o,
            None => PolynomialMutationArgs::default(problem.clone().as_ref()),
        };
        let mutation_operator = PolynomialMutation::new(mutation_options.clone())?;

        let crossover_options = options.crossover_operator_options.unwrap_or_default();
        let crossover_operator = SimulatedBinaryCrossover::new(crossover_options.clone())?;

        info!(
            "{}",
            NSGA2::algorithm_option_str(&problem, &crossover_options, &mutation_options)
        );

        Ok(Self {
            number_of_individuals,
            reference_points,
            ideal_point: vec![f64::INFINITY; problem.number_of_objectives()],
            population,
            problem,
            selector_operator,
            crossover_operator,
            mutation_operator,
            generation: 0,
            stopping_condition: options.stopping_condition,
            start_time: Instant::now(),
            parallel: options.parallel.unwrap_or(true),
            export_history: options.export_history,
            rng: get_rng(options.seed),
            args: nsga3_args,
        })
    }

    /// Get the normalised objective data stored in the `individual`
    ///
    /// # Arguments
    ///
    /// * `individual`: The individual reference with the data.
    ///
    /// returns: `Result<DataValue, OError>`
    fn get_normalised_objectives(individual: &Individual) -> Result<DataValue, OError> {
        individual.get_data(NORMALISED_OBJECTIVE_KEY)
    }

    /// For each reference point count selected individuals in P_{t+1} associated with it. This
    /// returns `rho_j` which is a lookup map, mapping the reference point index to the number of
    /// linked individuals.
    ///
    /// # Arguments
    ///
    /// * `selected_individuals`: The individuals to use to count the association to the reference
    /// points.
    /// * `reference_points`: The reference points.
    ///
    /// returns: `Result<HashMap<usize, usize>, OError>`
    pub fn get_association_map(
        selected_individuals: &Population,
        reference_points: &[Vec<f64>],
    ) -> Result<HashMap<usize, usize>, OError> {
        let mut rho_j: HashMap<usize, usize> = HashMap::new();
        for ind in selected_individuals.individuals() {
            let ref_point_index = ind.get_data(REF_POINT_INDEX);
            match ref_point_index {
                Ok(index) => {
                    let index = index.as_usize()?;
                    rho_j.entry(index).and_modify(|v| *v += 1).or_insert(1);
                }
                Err(_) => continue,
            }
        }
        // fill the rest
        for ref_point_index in 0..reference_points.len() {
            rho_j.entry(ref_point_index).or_insert(0);
        }

        Ok(rho_j)
    }

    /// Get the reference points used in the evolution.
    ///
    /// return: `Vec<Vec<f64>>`
    pub fn reference_points(&self) -> Vec<Vec<f64>> {
        self.reference_points.clone()
    }
}

/// Implementation of Section IV of the paper.
impl Algorithm<NSGA3Arg> for NSGA3 {
    /// This assesses the initial random population.
    ///
    /// return: `Result<(), OError>`
    fn initialise(&mut self) -> Result<(), OError> {
        info!("Evaluating initial population");
        if self.parallel {
            NSGA3::do_parallel_evaluation(self.population.individuals_as_mut())?;
        } else {
            NSGA3::do_evaluation(self.population.individuals_as_mut())?;
        }

        info!("Initial evaluation completed");
        self.generation += 1;

        Ok(())
    }

    /// Evolve the population. The first part of this code comes from NSGA2::evolve(). NSGA3 mainly
    /// differs in the selection method.
    fn evolve(&mut self) -> Result<(), OError> {
        // Create the new population, based on the population at the previous time-step, of size
        // self.number_of_individuals. The loop adds two individuals at the time.
        debug!("Generating new population (selection + crossover + mutation)");
        let mut offsprings: Vec<Individual> = Vec::new();
        for _ in 0..self.number_of_individuals / 2 {
            let parents =
                self.selector_operator
                    .select(self.population.individuals(), 2, &mut self.rng)?;

            // generate the 2 children with crossover
            let children = self.crossover_operator.generate_offsprings(
                &parents[0],
                &parents[1],
                &mut self.rng,
            )?;

            // mutate them
            offsprings.push(
                self.mutation_operator
                    .mutate_offspring(&children.child1, &mut self.rng)?,
            );
            offsprings.push(
                self.mutation_operator
                    .mutate_offspring(&children.child2, &mut self.rng)?,
            );
        }
        debug!("Combining parents and offsprings in new population");
        self.population.add_new_individuals(offsprings);
        debug!("New population size is {}", self.population.len());

        debug!("Evaluating population");
        if self.parallel {
            NSGA3::do_parallel_evaluation(self.population.individuals_as_mut())?;
        } else {
            NSGA3::do_evaluation(self.population.individuals_as_mut())?;
        }
        debug!("Evaluation done");

        debug!("Calculating fronts and ranks for new population");
        let sorting_results = fast_non_dominated_sort(self.population.individuals_as_mut(), false)?;
        debug!("Collected {} fronts", sorting_results.fronts.len());

        debug!("Selecting best individuals");
        // this is S_t in the paper, the population with the last front
        let mut new_population = Population::new();

        // Algorithm 1 in paper, step 5-7 - Fill the new population up to the last front.
        let mut last_front: Option<Vec<Individual>> = None;
        for (fi, front) in sorting_results.fronts.into_iter().enumerate() {
            if new_population.len() + front.len() <= self.number_of_individuals {
                // population does not overflow with new front
                debug!("Adding front #{} (size: {})", fi + 1, front.len());
                new_population.add_new_individuals(front);
            } else if new_population.len() == self.number_of_individuals {
                debug!("Population reached target size");
                break;
            } else {
                // Algorithm 1, step 12. Population is filled up to front l-1 (P_{t+1})
                debug!(
                    "Population almost full ({} individuals)",
                    new_population.len()
                );
                // this is F_l
                last_front = Some(front.clone());
                break;
            }
        }

        // Algorithm 1, step 11-18 - Complete the population using the members from the last front.
        if let Some(last_front) = last_front {
            // store the last population index containing individuals up to front F_l
            let first_dom_index = new_population.len();
            let missing_item_count = self.number_of_individuals - new_population.len();
            debug!("{missing_item_count} must be added from the last front");

            // add the last_front F_l to create S_t
            new_population.add_new_individuals(last_front);

            // Algorithm 1, step 14 - Calculate f_n
            debug!("Normalising all individuals");
            let mut norm =
                Normalise::new(&mut self.ideal_point, new_population.individuals_as_mut())?;
            norm.calculate()?;

            // Algorithm 1, step 15
            debug!("Associating reference points to all individuals");
            let mut assoc = AssociateToRefPoint::new(
                new_population.individuals_as_mut(),
                &self.reference_points,
            )?;
            assoc.calculate()?;

            // Algorithm 1, step 16
            // re-split population in P_{t+1} (S_t without the last front) and individuals in front F_l
            let mut potential_individuals = new_population.drain(first_dom_index..);
            // rename variable for clarity
            let mut selected_individuals = new_population;

            // for each reference point count selected individuals in P_{t+1} associated with it.
            // rho_j is a lookup map mapping the reference point index to the number of linked
            // individuals
            let mut rho_j =
                NSGA3::get_association_map(&selected_individuals, &self.reference_points)?;

            // Algorithm 4 - Niching
            debug!("Niching");
            let mut n = Niching::new(
                &mut selected_individuals,
                &mut potential_individuals,
                missing_item_count,
                &mut rho_j,
                &mut self.rng,
            )?;
            n.calculate()?;

            // update the population
            self.population = selected_individuals;
        } else {
            // update the population
            self.population = new_population;
        }

        self.generation += 1;
        Ok(())
    }

    fn generation(&self) -> usize {
        self.generation
    }

    fn name(&self) -> String {
        "NSGA3".to_string()
    }

    fn start_time(&self) -> &Instant {
        &self.start_time
    }

    fn stopping_condition(&self) -> &StoppingConditionType {
        &self.stopping_condition
    }

    fn population(&self) -> &Population {
        &self.population
    }

    fn problem(&self) -> Arc<Problem> {
        self.problem.clone()
    }

    fn export_history(&self) -> Option<&ExportHistory> {
        self.export_history.as_ref()
    }

    fn additional_export_data(&self) -> Option<HashMap<String, DataValue>> {
        let mut data = HashMap::new();
        let mut points: Vec<DataValue> = Vec::new();
        for point in self.reference_points() {
            points.push(DataValue::Vector(point));
        }
        data.insert(
            "reference_points".to_string(),
            DataValue::DataVector(points),
        );
        Some(data)
    }

    fn algorithm_options(&self) -> NSGA3Arg {
        self.args.clone()
    }
}

/// This implements "Algorithm 2" in the paper which normalises the population members using the
/// adaptive ideal point and the intercepts of the hyper-plane passing through the extreme points
/// and crossing the objective space axis. Steps 8-10 are ignored because [`NSGA3`] implementation
/// directly uses Das and Darren's approach with already-normalised points.
///
/// This procedure:
///  - updates the ideal point. The new coordinates may differ from the original point if any
///    objective, calculated at the current evolution, is lower than the one at the previous
///    evolution.
///  - store the normalised objective in the objective space, with respect to the new ideal point
///    and hyper-plane intercepts, in the "translated_objective" data key in each [`Individual`].
///    To retrieve those, use [`NSGA3::get_normalised_objectives()`].
pub struct Normalise<'a> {
    /// The coordinate of the ideal point from the previous evolution.
    ideal_point: &'a mut Vec<f64>,
    /// The individuals that need normalisation.
    individuals: &'a mut [Individual],
}

impl<'a> Normalise<'a> {
    /// Build the [`Normalise`] struct.
    ///
    /// # Arguments
    ///
    /// * `ideal_point`: The coordinate of the ideal point  from the previous evolution.
    /// * `individuals`: The individual that needs normalisation.
    ///
    /// returns: `Result<Normalise, OError>`
    pub fn new(
        ideal_point: &'a mut Vec<f64>,
        individuals: &'a mut [Individual],
    ) -> Result<Self, OError> {
        if individuals.is_empty() {
            return Err(OError::AlgorithmRun(
                "NSGA3-Normalise".to_string(),
                "The vector of individuals is empty".to_string(),
            ));
        }

        Ok(Normalise {
            ideal_point,
            individuals,
        })
    }

    /// Normalise the population members using "Algorithm 2" from the paper. Objectives are first
    /// translated with respect to the new ideal point and then scaled using the intercepts of the
    /// linear hyper-plane passing through the extreme points.
    ///
    /// # Arguments
    ///
    /// * `individuals`: The individuals with the objectives
    ///
    /// returns: `()`: this only updates the ideal point and stores the normalised objective in the
    /// [`Individual`]'s data.
    pub fn calculate(&mut self) -> Result<(), OError> {
        // Step 2 - calculate the new ideal point (based on paragraph IV-C), as the minimum value
        // for each objective from the start of the algorithm evolution up to the current evolution
        // step.
        let problem = self.individuals.first().unwrap().problem();
        for (j, obj_name) in problem.objective_names().iter().enumerate() {
            let new_min = vector_min(&self.individuals.objective_values(obj_name)?)?;
            // update the point if its coordinate is smaller
            if new_min < self.ideal_point[j] {
                self.ideal_point[j] = new_min;
            }
        }
        debug!("Set ideal point to {:?}", self.ideal_point);

        // Step 3 - Translate the individuals' objectives with respect to the ideal point. This
        // implements the calculation of `f'_j(x)` in section IV-C of the paper.
        for x in self.individuals.iter_mut() {
            let translated_objectives = x
                .get_objective_values()?
                .iter()
                .enumerate()
                .map(|(j, v)| v - self.ideal_point[j])
                .collect();
            debug!("Translated objective to {:?}", translated_objectives);
            x.set_data(
                NORMALISED_OBJECTIVE_KEY,
                DataValue::Vector(translated_objectives),
            );
        }

        // Step 4 - Calculate the vector of extreme points
        let mut extreme_points = vec![];
        for j in 0..problem.number_of_objectives() {
            // Extreme point z_j_max for current objective
            let mut weights = vec![10.0_f64.powi(-6); problem.number_of_objectives()];
            weights[j] = 1.0;

            let mut min_value = f64::INFINITY; // minimum ASF
            let mut ind_index = 0; // index of individual with minimum ASF
            for (x_idx, ind) in self.individuals.iter().enumerate() {
                let f_j = NSGA3::get_normalised_objectives(ind)?;
                let value = self.asf(f_j.as_f64_vec()?, &weights)?;
                if value < min_value {
                    min_value = value;
                    ind_index = x_idx;
                }
            }
            extreme_points.push(
                NSGA3::get_normalised_objectives(&self.individuals[ind_index])?
                    .as_f64_vec()?
                    .clone(),
            );
        }
        debug!("Set extreme points to {:?}", extreme_points);

        // Step 6 - Compute intercepts a_j with the least-square method
        let intercept_result = Self::calculate_plane_intercepts(&extreme_points)?;
        let intercepts: Vec<f64> = match intercept_result {
            None => {
                // no solution found or intercepts are too small - get worst (max) for each objective
                let max_points = self.calculate_max_objectives()?;
                debug!("Using maximum points as intercepts {:?}", max_points);
                max_points
            }
            Some(i) => {
                debug!("Found intercepts {:?}", i);
                i
            }
        };

        // Step 7 - Normalize objectives (f_n). The denominator differs from Eq. 5 in the paper
        // because the intercepts are already calculated using the translated objectives. The new
        // values are updated for all individuals.
        for individual in self.individuals.iter_mut() {
            let new_o: Vec<f64> = NSGA3::get_normalised_objectives(individual)?
                .as_f64_vec()?
                .iter()
                .enumerate()
                .map(|(oi, obj_value)| obj_value / intercepts[oi])
                .collect();
            debug!("Normalised objectives to {:?}", new_o);
            individual.set_data(NORMALISED_OBJECTIVE_KEY, DataValue::Vector(new_o));
        }

        Ok(())
    }

    /// Use the least square method to calculate the coefficients of the equation of the plane
    /// passing through the vector of `points`. For example, for a 3D system the equation being
    /// used is: $ax + by + cz = 1$. The coefficient vector $x = [a, b, c]$ is found by solving
    /// the linear system $A \cdot x = b$ where `A` is
    ///
    ///          | x_0   y_0   z_0 |
    ///      A = | x_1   y_1   z_1 |
    ///          |       ...       |
    ///          | x_n   y_n   z_n |
    /// `n` the size of `points` and $b = [1, 1, 1]$. The intercepts are then calculated as the
    /// inverse of `x` as $1/x$. For example for the z-axis intercept (with x=0 and y=0), the point
    /// is found by solving $cz = 1$ or $1/x\[2\]$.
    ///
    /// # Arguments
    ///
    /// * `points`: The point coordinates passing through the plane to calculate.
    ///
    /// returns: `Result<Vec<f64>, OError>`: The $ a_i $ intercept values for each axis (see Fig.2
    /// in the paper) or `None` if the intercepts are close to `0`.
    fn calculate_plane_intercepts(points: &[Vec<f64>]) -> Result<Option<Vec<f64>>, OError> {
        let b = vec![1.0; points.len()];
        let plane_coefficients =
            solve_linear_system(points, &b, Some(LinearSolverTolerance::default()))
                .map_err(|e| OError::AlgorithmRun("NSGA3-Normalise".to_string(), e))?;
        debug!("Plane coefficients {:?}", plane_coefficients);

        let intercepts: Vec<f64> = plane_coefficients.iter().map(|v| 1.0 / v).collect();

        // check that the intercepts are above the minimum threshold
        if intercepts.iter().all(|v| *v >= 10_f64.powi(-3)) {
            Ok(Some(intercepts))
        } else {
            Ok(None)
        }
    }

    /// Calculate the maximum value for each translated objective.
    ///
    /// return: `Result<Vec<f64>, OError>`
    fn calculate_max_objectives(&self) -> Result<Vec<f64>, OError> {
        let problem = self.individuals.first().unwrap().problem();
        let mut max_points = vec![];
        for j in 0..problem.number_of_objectives() {
            let mut obj_j_values = Vec::new();
            for ind in self.individuals.iter() {
                obj_j_values.push(NSGA3::get_normalised_objectives(ind)?.as_f64_vec()?[j]);
            }
            obj_j_values.push(f64::EPSILON);
            max_points.push(vector_max(&obj_j_values)?);
        }
        debug!("Using maximum points as intercepts {:?}", max_points);
        Ok(max_points)
    }

    /// Calculate the achievement scalarising function with weight vector `w`. This is Eq. 4 in the
    /// paper.
    ///
    /// # Arguments
    ///
    /// * `translated_objective`: The translated objective for an individual. This is f'_j(x).
    /// * `weights`: The weight vector.
    ///
    /// returns: `Result<Vec<f64>, OError>`
    fn asf(&self, translated_objective: &[f64], weights: &Vec<f64>) -> Result<f64, OError> {
        let asf: Vec<f64> = translated_objective
            .iter()
            .zip(weights)
            .map(|(x, w)| x / w)
            .collect();
        vector_max(&asf)
    }
}

/// This implements "Algorithm 3" in the paper which associates each individual's normalised
/// objectives to a reference point.
pub struct AssociateToRefPoint<'a> {
    /// The individuals containing the normalised objectives.
    individuals: &'a mut [Individual],
    /// The reference points
    reference_points: &'a [Vec<f64>],
}

impl<'a> AssociateToRefPoint<'a> {
    /// Build the [`AssociateToRefPoint`] structure. This returns an error if the reference point
    /// coordinates are not between 0 and 1.
    ///
    /// # Arguments
    ///
    /// * `individuals`: The individuals containing the normalised objectives.
    /// * `reference_points`: The reference points to associate the objectives to.
    ///
    /// returns: `Result<Self, OError>`
    pub fn new(
        individuals: &'a mut [Individual],
        reference_points: &'a [Vec<f64>],
    ) -> Result<Self, OError> {
        // check reference point values
        for point in reference_points {
            Self::check_bounds(point)?;
        }

        Ok(Self {
            individuals,
            reference_points,
        })
    }

    /// Associate the individuals to a reference point. If an association is found, this function
    /// stores the distance, the reference point coordinates and reference point index of
    /// [`self.reference_points`] in the individual's data.
    ///
    /// return `Result<(), OError>`
    pub fn calculate(&mut self) -> Result<(), OError> {
        // steps 1-3 are skipped because `reference_points` are already normalised

        // step 4-7
        for ind in self.individuals.iter_mut() {
            // fetch the data
            let data = NSGA3::get_normalised_objectives(ind)?;
            let obj_values = data.as_f64_vec()?;
            // calculate the distances for all reference points
            let d_per = self
                .reference_points
                .iter()
                .map(|ref_point| {
                    perpendicular_distance(ref_point, obj_values).map_err(|e| {
                        OError::AlgorithmRun(
                            "NSGA3-AssociateToRefPoint".to_string(),
                            format!("Cannot calculate vector distance because: {}", e),
                        )
                    })
                })
                .collect::<Result<Vec<f64>, OError>>()?;

            // step 8 - get the reference point with the lowest minimum distance
            let (ri, min_d) = argmin(&d_per);
            ind.set_data(MIN_DISTANCE, DataValue::Real(min_d));
            ind.set_data(
                REF_POINT,
                DataValue::Vector(self.reference_points[ri].clone()),
            );
            ind.set_data(REF_POINT_INDEX, DataValue::USize(ri));
            debug!(
                "Associated objective point {:?} to reference point #{} {:?} - distance = {}",
                ind.get_data(NORMALISED_OBJECTIVE_KEY)?,
                ri,
                self.reference_points[ri],
                min_d
            );
        }

        Ok(())
    }

    /// Check that the values in a reference point are between 0 and 1 (i.e. all the values have
    /// been normalised).
    ///
    /// # Arguments
    ///
    /// * `points`: The reference point coordinates to check.
    ///
    /// returns: `Result<(), OError>`
    fn check_bounds(points: &[f64]) -> Result<(), OError> {
        if points.iter().any(|v| !(0.0..=1.0).contains(v)) {
            return Err(OError::AlgorithmRun(
                "NSGA3-AssociateToRefPoint".to_string(),
                format!(
                    "The values of the reference point {:?} must be between 0 and 1",
                    points,
                ),
            ));
        }
        Ok(())
    }
}

/// This implements "Algorithm 4" in the paper which adds individuals from the last front to the new
/// population based on the reference point association and minimum distance.
pub struct Niching<'a> {
    /// The population being created at the current evolution with the new selected individuals.
    /// This is `$P_{t+1}$` from the paper and is populated with individuals from [`self.potential_individuals`].
    selected_individuals: &'a mut Population,
    /// Individuals from the last front $F_l$ to add to [`self.selected_individuals`] based on reference
    /// point association and minimum distance.
    potential_individuals: &'a mut Vec<Individual>,
    /// The number of individuals to add to [`self.selected_individuals`] to complete the evolution.
    missing_item_count: usize,
    /// The map mapping the reference point index to the number of individuals already associated in
    /// [`self.selected_individuals`]
    rho_j: &'a mut HashMap<usize, usize>,
    /// The random number generator
    rng: &'a mut Box<dyn RngCore>,
}

impl<'a> Niching<'a> {
    /// Niching algorithm.
    ///
    /// # Arguments
    ///
    /// * `selected_individuals`: The population P_{t+1} without the last front. This will be
    /// populated with individuals from `potential_individuals`.
    /// * `potential_individuals`: The potential individuals from the last front.
    /// * `number_of_individuals_to_add`: The number of individuals to add to `selected_individuals`
    /// from `potential_individuals`.
    /// * `rho_j`: The map containing the reference point indexes as keys and the number of associated
    /// points from P_{t+1}.
    /// * `rng`: The random number generator.
    ///
    /// returns: `Result<Niching, OError>`
    pub fn new(
        selected_individuals: &'a mut Population,
        potential_individuals: &'a mut Vec<Individual>,
        number_of_individuals_to_add: usize,
        rho_j: &'a mut HashMap<usize, usize>,
        rng: &'a mut Box<dyn RngCore>,
    ) -> Result<Self, OError> {
        if rho_j.is_empty() {
            return Err(OError::AlgorithmRun(
                "NSGA3-Niching".to_string(),
                "The rho_j set is empty".to_string(),
            ));
        }
        if potential_individuals.len() < number_of_individuals_to_add {
            return Err(OError::AlgorithmRun(
                "NSGA3-Niching".to_string(),
                format!("The number of individuals to add ({number_of_individuals_to_add}) is larger than the number of potential individuals ({})", potential_individuals.len()),
            ));
        }

        Ok(Self {
            selected_individuals,
            potential_individuals,
            missing_item_count: number_of_individuals_to_add,
            rho_j,
            rng,
        })
    }

    /// Add new individuals to the population. This updates [`self.new_population`] by draining
    /// items from [`self.potential_individuals`]. Reference points not associated with any point in
    /// [`self.potential_individuals`] and excluded from the current evolution are removed from
    /// [`self.rho_j`].
    ///
    /// return: `Result<(), OError>`
    pub fn calculate(&mut self) -> Result<(), OError> {
        let mut k = 1;
        let name = "NSGA3-Niching".to_string();
        debug!(
            "Number of individuals to choose {}",
            self.missing_item_count
        );
        while k <= self.missing_item_count {
            debug!(
                "Adding point {k}/{} to new population",
                self.missing_item_count
            );

            // step 3 - select the reference point with the minimum rho_j counter. Reference points
            // that have no association with individuals in F_l (Z_r = Z_r/{j_hat}, step 15) are
            // excluded by removing them from rho_j later
            let min_rho_j = *self
                .rho_j
                .iter()
                .min_by(|(_, v1), (_, v2)| v1.cmp(v2))
                .unwrap()
                .1;
            debug!("min_rho_j = {min_rho_j}");

            // step 3 - collect all reference point indexes j with minimum rho_j
            let j_min_set: Vec<usize> = self
                .rho_j
                .iter()
                .filter_map(|(ref_index, ref_counter)| {
                    if *ref_counter == min_rho_j {
                        Some(*ref_index)
                    } else {
                        None
                    }
                })
                .collect();

            // step 4 - get reference point with minimum association counter
            let j_hat = match j_min_set.len() {
                0 => {
                    return Err(OError::AlgorithmRun(
                        name.clone(),
                        "Empty j_min_set set".to_string(),
                    ))
                }
                1 => *j_min_set.first().unwrap(),
                // select point randomly when set size is > 1
                _ => *j_min_set.choose(self.rng.as_mut()).unwrap(),
            };
            debug!("Selected reference point j_hat=#{j_hat}");

            // step 5 - individuals in F_j linked to current reference point index j_hat
            let i_j: Vec<&Individual> = self
                .potential_individuals
                .iter()
                .filter(|ind| ind.get_data(REF_POINT_INDEX).unwrap() == DataValue::USize(j_hat))
                .collect();
            debug!(
                "Found {} potential individuals associated with it",
                i_j.len()
            );

            // step 6 - select points from front F_l
            if !i_j.is_empty() {
                let (new_ind_index, method) = if min_rho_j == 0 {
                    // step 7 - no point from P_{t+1} is associated with the selected reference point
                    // j_hat. There's at least one point from F_l that can be linked (I_j is not empty)

                    // step 8 - find individual in F_l with the shortest distance
                    let (ij_ind_index, _) = argmin_by(&i_j, |(_, ind)| {
                        ind.get_data(MIN_DISTANCE).unwrap().as_real().unwrap()
                    })
                    .unwrap();
                    let new_ind_index = index_of(self.potential_individuals, i_j[ij_ind_index]);
                    (new_ind_index, "min_distance")
                } else {
                    // step 10 - choose random point from F_l
                    let ind = i_j.choose(self.rng).unwrap();
                    let new_ind_index = index_of(self.potential_individuals, ind);
                    (new_ind_index, "random")
                };
                // step 12a - mark reference point as associated to a new F_l's individual
                *self.rho_j.get_mut(&j_hat).unwrap() += 1;

                // step 12b - Add new individual and remove it from F_l
                match new_ind_index {
                    None => {
                        return Err(OError::AlgorithmRun(
                            name,
                            "Cannot find individual's index".to_string(),
                        ))
                    }
                    Some(index) => {
                        let ind = self.potential_individuals.remove(index);
                        debug!(
                            "Added individual #{index} {:?} to population ({method}) - reference point #{j_hat} {:?}",
                            ind.get_data(NORMALISED_OBJECTIVE_KEY)?,
                            ind.get_data(REF_POINT),
                        );
                        self.selected_individuals.add_individual(ind);

                        // step 13
                        k += 1;
                    }
                }
            } else {
                // step 15 - no point in F_l is associated with reference point indexed by j_hat.
                // j_hat will have no linked individual at this evolution. Excluding it.
                debug!("Excluding ref point index {j_hat} - no candidates associated with it");
                self.rho_j.remove(&j_hat);
            }
        }

        Ok(())
    }
}

#[cfg(test)]
mod test_algorithms {
    use std::collections::HashMap;

    use float_cmp::assert_approx_eq;

    use crate::algorithms::Normalise;
    use crate::algorithms::nsga3::{
        AssociateToRefPoint, MIN_DISTANCE, Niching, NORMALISED_OBJECTIVE_KEY, REF_POINT,
        REF_POINT_INDEX,
    };
    use crate::core::{DataValue, Individual, ObjectiveDirection, Population};
    use crate::core::test_utils::{assert_approx_array_eq, individuals_from_obj_values_dummy};
    use crate::core::utils::get_rng;
    use crate::utils::{DasDarren1998, NumberOfPartitions};

    #[test]
    /// Test intercepts. Points were generated from numpy from uniform distribution with normal
    /// distributed noise on z coordinates (scale=1). Plane was generated to have slope of -2 in
    /// the x direction and -3 in the y direction.
    fn test_intercepts() {
        let points = vec![
            vec![3.3817863, 0.40604364, -2.2899773],
            vec![4.1741924, 0.92094903, -5.91434001],
            vec![3.42070899, 0.90266942, -3.81063094],
            vec![1.11301849, 0.94849208, 0.17140235],
            vec![9.08303894, 0.74599477, -16.14020622],
            vec![0.98976491, 0.84847939, 0.82864021],
            vec![7.53579489, 0.73723563, -11.72284018],
            vec![6.96274164, 0.59449793, -10.71963907],
            vec![5.60255823, 1.69973452, -12.49841699],
            vec![6.16815342, 0.66601692, -11.63169056],
        ];
        let intercepts = Normalise::calculate_plane_intercepts(&points)
            .unwrap()
            .unwrap();
        assert_approx_array_eq(&intercepts, &[3.38096778, 1.61009025, 7.58962871]);
    }

    #[test]
    /// Test `AssociateToRefPoint` that calculates the correct distances and reference point
    /// association.
    fn test_simple_association() {
        let das_darren = DasDarren1998::new(3, &NumberOfPartitions::OneLayer(4)).unwrap();
        let ref_points = das_darren.get_weights();

        let dummy_objectives = vec![vec![0.0, 0.0], vec![50.0, 50.0]];
        let mut individuals = individuals_from_obj_values_dummy(
            &dummy_objectives,
            &[ObjectiveDirection::Minimise, ObjectiveDirection::Minimise],
            None,
        );
        // set normalised objectives
        individuals[0].set_data(
            NORMALISED_OBJECTIVE_KEY,
            DataValue::Vector(vec![0.95, 0.15, 0.15]),
        );
        individuals[1].set_data(
            NORMALISED_OBJECTIVE_KEY,
            DataValue::Vector(vec![0.1, 0.9, 0.1]),
        );

        let mut ass = AssociateToRefPoint::new(&mut individuals, &ref_points).unwrap();
        ass.calculate().unwrap();

        // 1st individual
        assert_approx_array_eq(
            individuals[0]
                .get_data(REF_POINT)
                .unwrap()
                .as_f64_vec()
                .unwrap(),
            &[1.0, 0.0, 0.0],
        );
        assert_approx_eq!(
            f64,
            individuals[0]
                .get_data(MIN_DISTANCE)
                .unwrap()
                .as_real()
                .unwrap(),
            0.212132034355,
            epsilon = 0.0001
        );

        // 2nd individual
        assert_approx_array_eq(
            individuals[1]
                .get_data(REF_POINT)
                .unwrap()
                .as_f64_vec()
                .unwrap(),
            &[0.0, 1.0, 0.0],
        );
        assert_approx_eq!(
            f64,
            individuals[1]
                .get_data(MIN_DISTANCE)
                .unwrap()
                .as_real()
                .unwrap(),
            0.1414213562,
            epsilon = 0.0001
        );
    }

    #[test]
    /// Check niching that (1) adds point with min distance when there reference point is not
    /// already associated with an objective; (2) reference points not linked to potential individuals
    /// are excluded from the algorithm.
    fn test_niching_rho0() {
        // create dummy population with 4 individuals
        let dummy_objectives = vec![vec![0.0, 0.0]; 2];
        let mut individuals = individuals_from_obj_values_dummy(
            &dummy_objectives,
            &[ObjectiveDirection::Minimise; 2],
            None,
        );
        let problem = individuals[0].problem().clone();
        let mut rho_j: HashMap<usize, usize> = HashMap::new();

        // link 2 individuals to 2 out of 4 reference points
        individuals[0].set_data(REF_POINT_INDEX, DataValue::USize(0));
        individuals[0].set_data(MIN_DISTANCE, DataValue::Real(0.1));
        rho_j.entry(0).or_insert(1);

        individuals[1].set_data(REF_POINT_INDEX, DataValue::USize(1));
        individuals[1].set_data(MIN_DISTANCE, DataValue::Real(0.2));
        rho_j.entry(1).or_insert(1);
        let mut pop = Population::new_with(individuals);

        // potential individuals - both are linked to ref_point #3 but ind_3 is closer
        let mut ind_3 = Individual::new(problem.clone());
        ind_3.set_data(REF_POINT_INDEX, DataValue::USize(2));
        ind_3.set_data(MIN_DISTANCE, DataValue::Real(0.4));

        let mut ind_4 = Individual::new(problem);
        ind_4.set_data(REF_POINT_INDEX, DataValue::USize(2));
        ind_4.set_data(MIN_DISTANCE, DataValue::Real(0.9));

        // counter is 0 for all other ref_points
        rho_j.entry(2).or_insert(0);
        rho_j.entry(3).or_insert(0);
        let mut potential_individuals = vec![ind_3, ind_4];
        let selected_ind = potential_individuals[0].clone();

        let mut rng = get_rng(Some(1));
        let mut n = Niching::new(
            &mut pop,
            &mut potential_individuals,
            1,
            &mut rho_j,
            &mut rng,
        )
        .unwrap();
        n.calculate().unwrap();
        // let excluded = n.excluded_ref_point_index;

        // counter for ref_point #3 has increased
        assert_eq!(rho_j[&2_usize], 1_usize);
        // 3rd individual is added to the population
        assert_eq!(pop.len(), 3);
        assert_eq!(pop.individual(2).unwrap(), &selected_ind);
        // 4th reference point should be excluded because has no association
        // NOTE: do not check this due to random ref_point selection (if point 4 is picked first,
        // this is excluded otherwise it will not).
        // assert!(
        //     !rho_j.contains(&3_usize),
        //     "excluded = {:?} does not contain ref_point #4",
        //     rho_j
        // );
    }
    #[test]
    /// Check niching that adds point with min distance when there reference point is already
    /// associated with another objective.
    fn test_niching_rho1() {
        // create dummy population with 4 individuals
        let dummy_objectives = vec![vec![0.0, 0.0]; 2];
        let mut individuals = individuals_from_obj_values_dummy(
            &dummy_objectives,
            &[ObjectiveDirection::Minimise; 2],
            None,
        );
        let problem = individuals[0].problem().clone();
        let mut rho_j: HashMap<usize, usize> = HashMap::new();

        // link 2 individuals to 2 out of 4 reference points
        individuals[0].set_data(REF_POINT_INDEX, DataValue::USize(0));
        individuals[0].set_data(MIN_DISTANCE, DataValue::Real(0.1));
        rho_j.entry(0).or_insert(1);

        individuals[1].set_data(REF_POINT_INDEX, DataValue::USize(1));
        individuals[1].set_data(MIN_DISTANCE, DataValue::Real(0.2));
        rho_j.entry(1).or_insert(1);
        let mut pop = Population::new_with(individuals);

        // potential individuals - both are linked to ref_point #2 but ind_4 is closer
        let mut ind_3 = Individual::new(problem.clone());
        ind_3.set_data(REF_POINT_INDEX, DataValue::USize(1));
        ind_3.set_data(MIN_DISTANCE, DataValue::Real(99.0));

        let mut ind_4 = Individual::new(problem);
        ind_4.set_data(REF_POINT_INDEX, DataValue::USize(1));
        ind_4.set_data(MIN_DISTANCE, DataValue::Real(0.9));

        // counter is 0 for all other ref_points
        rho_j.entry(2).or_insert(0);
        let mut potential_individuals = vec![ind_3, ind_4];
        let selected_ind = potential_individuals[1].clone();

        let mut rng = get_rng(Some(1));
        let mut n = Niching::new(
            &mut pop,
            &mut potential_individuals,
            1,
            &mut rho_j,
            &mut rng,
        )
        .unwrap();
        n.calculate().unwrap();

        // counter for ref_point #3 has increased
        assert_eq!(rho_j[&1_usize], 2_usize);
        // 3rd individual is added to the population
        assert_eq!(pop.len(), 3);
        assert_eq!(pop.individual(2).unwrap(), &selected_ind);
    }
}

#[cfg(test)]
mod test_problems {
    use crate::algorithms::{
        Algorithm, MaxGeneration, NSGA3, NSGA3Arg, Nsga3NumberOfIndividuals, StoppingConditionType,
    };
    use crate::core::builtin_problems::dtlz1;
    use crate::core::test_utils::check_exact_value;
    use crate::operators::{PolynomialMutationArgs, SimulatedBinaryCrossoverArgs};
    use crate::utils::NumberOfPartitions;

    #[test]
    /// Test the ZTD1 problem from Deb et al. (2013) with M=3 (see Table III).
    fn test_ztd1_problem_1() {
        // see Table I
        let number_objectives: usize = 3;
        let k: usize = 5;
        let number_variables: usize = number_objectives + k - 1; // M + k - 1 with k = 5 (Section Va)
        let problem = dtlz1(number_variables, number_objectives).unwrap();
        // The number of partitions used in the paper when M=3 - Table I
        let number_of_partitions = NumberOfPartitions::OneLayer(12);

        // see Table II
        let crossover_operator_options = SimulatedBinaryCrossoverArgs {
            distribution_index: 30.0,
            crossover_probability: 1.0,
            variable_probability: 1.0,
        };
        // eta_m = 20 - probability  1/n_vars
        let mutation_operator_options = PolynomialMutationArgs::default(&problem);

        let args = NSGA3Arg {
            // see Table I
            number_of_individuals: Nsga3NumberOfIndividuals::Custom(92),
            number_of_partitions,
            crossover_operator_options: Some(crossover_operator_options),
            mutation_operator_options: Some(mutation_operator_options),
            // see Table III
            stopping_condition: StoppingConditionType::MaxGeneration(MaxGeneration(400)),
            parallel: None,
            export_history: None,
            seed: Some(1),
        };

        let mut algo = NSGA3::new(problem, args).unwrap();
        assert_eq!(algo.reference_points().len(), 91);

        algo.run().unwrap();
        let results = algo.get_results();

        // All objective points lie on the plane passing through the 0.5 intercept on each axis (i.e.
        // the sum of the objective coordinate is close to 0.5). Because of randomness a few solutions
        // may breach this condition.
        let obj_sum: Vec<f64> = results
            .individuals
            .iter()
            .map(|ind| ind.get_objective_values().unwrap().iter().sum())
            .collect();
        let strict_range = 0.47..0.53;
        let loose_range = -10.0..10.0;
        let (x_other_outside_bounds, breached_range, b_type) =
            check_exact_value(&obj_sum, &strict_range, &loose_range, 2);
        if !x_other_outside_bounds.is_empty() {
            panic!(
                "Found {} objectives ({:?}) outside the {} bounds {:?}",
                x_other_outside_bounds.len(),
                x_other_outside_bounds,
                b_type,
                breached_range
            );
        }
    }
}
